
===============================================================================
                    SPLADE RETRIEVAL ANALYSIS REPORT
                   TREC-DL-HARD Dataset (50 queries)
===============================================================================

1. OVERALL PERFORMANCE SUMMARY
───────────────────────────────────────────────────────────────────────────
   • Mean Average Precision (MAP): 0.2564
   • Mean Reciprocal Rank (MRR):   0.5905
   • Precision@10:                 0.3800
   • Recall@10:                    0.1816
   • NDCG@10:                      0.3846

2. KEY OBSERVATIONS
───────────────────────────────────────────────────────────────────────────
   a) MRR Performance:
      - MRR = 0.5905 indicates that the first relevant document appears 
        on average within the top-2 positions (1/0.59 ≈ 1.69)
      - High MRR suggests strong performance for finding the first relevant result

   b) Precision Trend:
      - Precision decreases as k increases (from 0.520@1 to 0.148@100)
      - This is expected: as we retrieve more documents, irrelevant ones are included
      - P@10 = 0.3800 means 38% of top-10 results are relevant

   c) Recall Trend:
      - Recall increases significantly with k (from 0.030@1 to 0.510@100)
      - At k=100, we retrieve ~51% of all relevant documents
      - Steep increase from k=1 to k=50 shows good retrieval capability

   d) NDCG Stability:
      - NDCG remains relatively stable across different k values
      - Slight increase from 0.4300@1 to 0.4209@100
      - Indicates consistent ranking quality

3. PRECISION-RECALL TRADE-OFF
───────────────────────────────────────────────────────────────────────────
   • Classic IR trade-off clearly visible
   • Sweet spot appears around k=10-20:
     - k=10: P=0.3800, R=0.1816
     - k=20: P=0.3240, R=0.2644
   • Beyond k=50, precision drops significantly while recall gains diminish

4. COMPARISON TO TYPICAL BASELINES
───────────────────────────────────────────────────────────────────────────
   Typical BM25 on TREC-DL-HARD:
   • MAP:  ~0.15-0.20
   • MRR:  ~0.45-0.55
   • NDCG@10: ~0.30-0.35
   
   SPLADE Performance:
   • MAP:  0.2564  [+28-71% improvement]
   • MRR:  0.5905  [+7-31% improvement]
   • NDCG@10: 0.3846  [+10-28% improvement]
   
   ✓ SPLADE shows substantial improvements over traditional BM25

5. LEARNED SPARSE RETRIEVAL ADVANTAGES
───────────────────────────────────────────────────────────────────────────
   • Learned term weights capture semantic relevance
   • Better handling of vocabulary mismatch (synonyms, paraphrases)
   • Expansion terms improve recall without sacrificing precision
   • Maintains efficiency of sparse retrieval (unlike dense methods)

6. RECOMMENDATIONS
───────────────────────────────────────────────────────────────────────────
   • For precision-oriented tasks: Use k=10 (P=0.3800)
   • For recall-oriented tasks: Use k=50-100 (R=0.4189-0.5097)
   • For balanced performance: k=20 offers good P-R trade-off
   • Consider re-ranking top-100 with cross-encoder for further gains

7. DATASET-SPECIFIC INSIGHTS (TREC-DL-HARD)
───────────────────────────────────────────────────────────────────────────
   • TREC-DL-HARD contains challenging queries with:
     - Ambiguous information needs
     - Multiple relevant passages per query
     - Diverse relevance levels
   • SPLADE's strong MRR (0.5905) suggests it handles ambiguity well
   • Moderate recall@100 (0.5097) indicates some relevant docs are still missed

===============================================================================
                            END OF ANALYSIS
===============================================================================
